# FastForth Llama CLI

AI-powered Forth development assistant using FastForth and Ollama.

## Quick Start

### Using the Shell Wrapper (Works Now)

```bash
# Build FastForth (if not already built)
cd /Users/joshkornreich/Documents/Projects/Ollama/llama/variants/fast-forth
cargo build --release

# Run the Llama CLI
./bin/fastforth-llama "What is recursion?"

# Interactive mode
./bin/fastforth-llama -i

# Use different model
./bin/fastforth-llama -m codellama "Explain Forth stack operations"

# Read prompt from file
echo "Explain JIT compilation" > prompt.txt
./bin/fastforth-llama -f prompt.txt
```

### Installation (Optional)

To install globally:

```bash
# Copy to a directory in your PATH
sudo cp bin/fastforth-llama /usr/local/bin/

# Or create a symlink
ln -s "$(pwd)/bin/fastforth-llama" ~/bin/fastforth-llama

# Then use from anywhere
fastforth-llama "your prompt here"
```

## How It Works

### Current Implementation (Hybrid Wrapper)

The current `bin/fastforth-llama` is a **shell wrapper** that:

1. Uses **FastForth** for Forth code execution (JIT compilation)
2. Uses **curl** for HTTP communication with Ollama
3. Handles JSON building and parsing in bash
4. Provides a clean CLI interface

**Architecture:**
```
User Input
    â†“
fastforth-llama (bash script)
    â†“
    â”œâ”€â†’ FastForth (for Forth computation)
    â””â”€â†’ curl (for HTTP to Ollama)
            â†“
        Ollama API
            â†“
        AI Response
```

**Why a wrapper?**
- FastForth doesn't have FFI support yet (no file I/O, system calls)
- This provides a working solution TODAY
- Demonstrates the concept and user experience
- Will be replaced when FFI is implemented

### Future Implementation (Native FFI)

When FFI support is added to FastForth, we'll have a **pure Forth** implementation:

1. `examples/llama-cli.fth` - The future native version
2. Direct HTTP calls from Forth code
3. File I/O for temp files
4. Command-line argument parsing in Forth

**See:** `LLAMA_CLI_PORT_ROADMAP.md` for the implementation plan.

## Usage Examples

### Basic Query

```bash
$ ./bin/fastforth-llama "What is recursion in Forth?"
Recursion in Forth is when a word calls itself. Here's an example...
[AI response continues]
```

### Code Explanation

```bash
$ ./bin/fastforth-llama "Explain this Forth code: : factorial dup 1 > if dup 1 - factorial * else drop 1 then ;"

This code defines a factorial function using recursion:
1. dup - Duplicates the input number
2. 1 > - Checks if it's greater than 1
3. if - If true, recurse: dup 1 - factorial *
4. else - Base case: drop the number and return 1
...
```

### Interactive Mode

```bash
$ ./bin/fastforth-llama -i
FastForth Llama - Interactive Mode
Type your prompts. Commands: /exit /help /model <name>

> What is a stack effect?
A stack effect describes what a Forth word does to the stack...

> /model codellama
Switched to model: codellama

> Write a Forth function to reverse a string
: reverse-string ( addr len -- reversed-addr reversed-len )
  ...

> /exit
Goodbye!
```

### Different Models

```bash
# Use CodeLlama for code-focused questions
./bin/fastforth-llama -m codellama "Write a Forth quicksort"

# Use llama3.2 for general questions
./bin/fastforth-llama -m llama3.2 "Explain compiler optimizations"
```

## Configuration

### Environment Variables

```bash
# Set Ollama host (default: http://localhost:11434)
export OLLAMA_HOST="http://192.168.1.100:11434"

# Set default model (default: llama3.2)
export OLLAMA_MODEL="codellama"

# Then use
./bin/fastforth-llama "your prompt"
```

### Command-Line Options

```
Options:
  -m MODEL    Set Ollama model (default: llama3.2)
  -h HOST     Set Ollama host (default: http://localhost:11434)
  -v          Verbose mode (show debug output)
  -f FILE     Read prompt from file
  -i          Interactive mode
  --help      Show help
```

## Dependencies

### Required

- **curl** - For HTTP requests to Ollama
- **bash** - For the wrapper script

### Optional

- **jq** - For better JSON parsing (falls back to grep if not available)

### Install Dependencies

```bash
# macOS
brew install curl jq

# Linux (Debian/Ubuntu)
sudo apt-get install curl jq

# Linux (RHEL/CentOS)
sudo yum install curl jq
```

## Ollama Setup

Make sure Ollama is running:

```bash
# Start Ollama
ollama serve

# Pull a model (if not already done)
ollama pull llama3.2
ollama pull codellama  # For code-focused queries
```

## Comparison with gforth Version

| Feature | gforth llama-forth | FastForth llama-cli |
|---------|-------------------|-------------------|
| Startup time | ~50ms | ~10ms (FastForth JIT) |
| Forth execution | Interpreted | JIT compiled |
| HTTP client | curl (system call) | curl (wrapper) â†’ FFI (future) |
| File I/O | Native gforth | Shell wrapper â†’ FFI (future) |
| JSON parsing | Manual string ops | Manual â†’ library (future) |
| Compilation | No compilation | Cranelift JIT |
| Performance | Slower | Faster (especially for compute) |

**Key Insight:** FastForth's JIT compilation shines for compute-intensive Forth code, but currently relies on shell wrapper for I/O. Future FFI support will enable pure-Forth implementation.

## Development Status

### âœ… Working Now

- [x] Shell wrapper CLI
- [x] Basic Ollama API calls via curl
- [x] Interactive mode
- [x] Multiple models support
- [x] File input mode
- [x] Error handling
- [x] Verbose mode for debugging

### ğŸš§ Planned (Requires FFI)

- [ ] Native FFI support in FastForth
- [ ] File I/O words (create-file, read-file, write-file)
- [ ] System call support (system word)
- [ ] Command-line argument parsing in Forth
- [ ] Pure Forth HTTP client
- [ ] Native JSON builder/parser
- [ ] Streaming responses
- [ ] Local variables syntax ({ })

**See:** `LLAMA_CLI_PORT_ROADMAP.md` for detailed implementation plan.

## Architecture

### Current (Hybrid)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   bin/fastforth-llama (bash)        â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  FastForth   â”‚  â”‚    curl     â”‚ â”‚
â”‚  â”‚ (Computation)â”‚  â”‚   (HTTP)    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                 â”‚
           â”‚                 â””â”€â†’ Ollama API
           â”‚
           â””â”€â†’ JIT-compiled Forth code
```

### Future (Native FFI)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   fastforth-llama (compiled binary) â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   FastForth Runtime          â”‚  â”‚
â”‚  â”‚   - FFI layer                â”‚  â”‚
â”‚  â”‚   - HTTP client (libc)       â”‚  â”‚
â”‚  â”‚   - File I/O (libc)          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â””â”€â†’ Direct libc calls
                  â”‚
                  â””â”€â†’ Ollama API
```

## Performance

### Wrapper Overhead

The shell wrapper adds minimal overhead:
- Bash startup: ~5ms
- curl startup: ~10ms
- Total CLI overhead: ~15ms

The Ollama API call dominates (200-2000ms), so wrapper overhead is negligible.

### FastForth JIT Performance

For Forth code execution:
- Compilation: ~50ms (Cranelift)
- Execution: Near-native speed (70-90% of C)

**Example:** Computing factorial(20) in a loop:
- gforth: ~100ms (interpreted)
- FastForth: ~8ms (JIT compiled)
- C: ~5ms (AOT compiled)

## Troubleshooting

### "curl: command not found"

Install curl:
```bash
brew install curl  # macOS
sudo apt-get install curl  # Linux
```

### "Error: Ollama API returned HTTP 000"

Ollama is not running. Start it:
```bash
ollama serve
```

### "Error: Failed to build FastForth"

Make sure Rust is installed:
```bash
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
```

### "Warning: FastForth binary not found"

Build FastForth first:
```bash
cd /Users/joshkornreich/Documents/Projects/Ollama/llama/variants/fast-forth
cargo build --release
```

### JSON parsing issues

Install jq for better JSON handling:
```bash
brew install jq  # macOS
sudo apt-get install jq  # Linux
```

## Contributing

### Adding Features to the Wrapper

The wrapper is a bash script at `bin/fastforth-llama`. To add features:

1. Edit the script
2. Test with various prompts
3. Update this README

### Working on Native FFI Implementation

See `LLAMA_CLI_PORT_ROADMAP.md` for the FFI implementation plan:

1. Implement file I/O words
2. Implement system() word
3. Add string utilities
4. Port `examples/llama-cli.fth` to use FFI
5. Compile to standalone binary

## References

- FastForth JIT compiler: `../README.md`
- FFI implementation plan: `LLAMA_CLI_PORT_ROADMAP.md`
- Ollama API: https://github.com/ollama/ollama/blob/main/docs/api.md
- gforth llama client: `/Users/joshkornreich/Documents/Projects/Ollama/llama/`

## License

Same as FastForth project.

## Authors

- FastForth: (see main README)
- Llama CLI wrapper: Created by Claude Code (2025-11-15)

---

**Note:** This is a transitional solution. The goal is to replace the shell wrapper with a pure-Forth implementation once FFI support is added to FastForth. The wrapper demonstrates the concept and provides immediate utility while the native implementation is being developed.
